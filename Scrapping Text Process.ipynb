{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99a77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "service_obj = Service(\"C:\\\\Users\\\\rashp\\\\anaconda3\\\\lib\\\\site-packages\\\\chromedriver_binary\\\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(service=service_obj) \n",
    "df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c94717",
   "metadata": {},
   "source": [
    "# Simple process for extracting the text from any given link(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a5be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 114/114 [09:25<00:00,  4.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Func to extract data(text) 'only text' from the given links(n no. of links) in the datafile 'Input.xlsx' \n",
    "\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    link = df['URL'][i]       #URL is the column name having links \n",
    "    browser.get(link)         #now browser open the link\n",
    "    time.sleep(1)             #now wait for 1 sec so that page load fully\n",
    "    soup = BeautifulSoup(browser.page_source , 'html.parser')      #soup now have all the data of that page and pass html parser  \n",
    "    try:\n",
    "        d = (soup.find('div' , class_ = 'td-pb-span8 td-main-content').find('div' , class_ = 'td-post-content').text.strip())\n",
    "        d = d.replace('\\n\\n', ' ')\n",
    "        d = d.replace('\\n' , ' ')                        # it fetches the data only text data and replaces '\\n', '\\n\\n' with ' space' to make corps data more readable\n",
    "        fd = open(str(df['URL_ID'][i]), 'w')             # now file is created and open in a write format\n",
    "        fd.write(d)                                      # and write the content of 'd' in the file\n",
    "        fd.close()                                       # now close the file..imp to close  \n",
    "    except:\n",
    "        fd = open(str(df['URL_ID'][i]), 'w')             # if in case we didn't find text there( as we get error in opening that page i.e page not found or any other error )then it just saves the given text and continue\n",
    "        fd.write('page error..Content Not Found')\n",
    "        fd.close()                                       \n",
    "        \n",
    "        # and now this process occur for each n every link you have in ur Input file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
